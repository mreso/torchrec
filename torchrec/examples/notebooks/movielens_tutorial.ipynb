{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchrec MovieLens Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget https://files.grouplens.org/datasets/movielens/ml-25m.zip\n",
    "# %load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. Instantiating MovieLens-25M dataset\n",
    "2. Defining model\n",
    "3. Training and evaluating model\n",
    "4. Finding similar movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instantiating MovieLens-25M dataset\n",
    "\n",
    "To start, we can load the MovieLens-25M dataset using `torchrec.datasets.movielens.movielens_25m`. The function loads just the user-movie ratings data in `ratings.csv` by default; we call the function with `include_movies_data=True` such that it adds movie data from `movies.csv` to each user-movie sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchrec.datasets.movielens import movielens_25m\n",
    "\n",
    "# dp = movielens_25m(\"ml-25m\", include_movies_data=True)\n",
    "from movielens import movielens_25m\n",
    "dp = movielens_25m(\"s3://torchrec-movielens-dataset/\", include_movies_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'userId': 1,\n",
       " 'movieId': 296,\n",
       " 'rating': 5.0,\n",
       " 'timestamp': 1147880044,\n",
       " 'title': 'Pulp Fiction (1994)',\n",
       " 'genres': 'Comedy|Crime|Drama|Thriller'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems reasonable.\n",
    "\n",
    "Next, we instantiate datapipes representing training and validation data splits and apply shuffling and batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrec.datasets.utils import rand_split_train_val\n",
    "\n",
    "train_dp, val_dp = rand_split_train_val(dp, 0.9)\n",
    "batched_train_dp = train_dp.shuffle(buffer_size=int(1e5)).batch(8192)\n",
    "batched_val_dp = val_dp.batch(8192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out that the integer user ids and movie ids referenced by the dataset aren't contiguous. Let's remap them to contiguous values so that we can use them with `torch.nn.Embedding` more easily downstream.\n",
    "\n",
    "To do so, we first populate dictionaries that map movie and user ids to ids in contiguous ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contig_movie_ids = {}\n",
    "# contig_user_ids = {}\n",
    "# movie_id_to_title_genre = {}\n",
    "\n",
    "# available_movie_id = 0\n",
    "# available_user_id = 0\n",
    "# for sample in dp:\n",
    "#     if sample[\"movieId\"] not in contig_movie_ids:\n",
    "#         contig_movie_ids[sample[\"movieId\"]] = available_movie_id\n",
    "#         available_movie_id += 1\n",
    "#     if sample[\"userId\"] not in contig_user_ids:\n",
    "#         contig_user_ids[sample[\"userId\"]] = available_user_id\n",
    "#         available_user_id += 1\n",
    "#     movie_id_to_title_genre[sample[\"movieId\"]] = (sample[\"title\"], sample[\"genres\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", and then define a function `_transform` that uses those dictionaries to remap movie and user ids for a batch of data. While we're at it, we'll also have `_transform` reformat the batch as tensors representing user ids, movie ids, and labels (numerical movie ratings given by users)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_user_ids_num = 162541\n",
    "unique_movie_ids_num = 59047"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from threading import Lock\n",
    "\n",
    "class Transform(object):\n",
    "    def __init__(self):\n",
    "        self._contig_movie_ids = {}\n",
    "        self._contig_user_ids = {}\n",
    "        self._movie_id_to_title_genre = {}\n",
    "        \n",
    "        self._lock = Lock()\n",
    "        self._available_movie_id = 0\n",
    "        self._available_user_id = 0\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        self._update_ids(batch)\n",
    "        return self._transform(batch)\n",
    "        \n",
    "    def _update_ids(self, batch):\n",
    "        with self._lock:\n",
    "            for sample in batch:\n",
    "                if sample[\"movieId\"] not in self._contig_movie_ids:\n",
    "                    self._contig_movie_ids[sample[\"movieId\"]] = self._available_movie_id\n",
    "                    self._available_movie_id += 1\n",
    "                if sample[\"userId\"] not in self._contig_user_ids:\n",
    "                    self._contig_user_ids[sample[\"userId\"]] = self._available_user_id\n",
    "                    self._available_user_id += 1\n",
    "                self._movie_id_to_title_genre[sample[\"movieId\"]] = (sample[\"title\"], sample[\"genres\"])\n",
    "    \n",
    "    def _transform(self, batch):\n",
    "        user_ids = torch.tensor([self._contig_user_ids[sample[\"userId\"]] for sample in batch], dtype=torch.int32)\n",
    "        movie_ids = torch.tensor([self._contig_movie_ids[sample[\"movieId\"]] for sample in batch], dtype=torch.int32)\n",
    "        labels = torch.tensor([sample[\"rating\"] for sample in batch], dtype=torch.float)\n",
    "        return user_ids, movie_ids, labels\n",
    "            \n",
    "\n",
    "# def _transform(batch):\n",
    "#     user_ids = torch.tensor([contig_user_ids[sample[\"userId\"]] for sample in batch], dtype=torch.int32)\n",
    "#     movie_ids = torch.tensor([contig_movie_ids[sample[\"movieId\"]] for sample in batch], dtype=torch.int32)\n",
    "#     labels = torch.tensor([sample[\"rating\"] for sample in batch], dtype=torch.float)\n",
    "#     return user_ids, movie_ids, labels\n",
    "\n",
    "_transform = Transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we configure our training and validation datapipes to apply `_transform` to each batch of data using `map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_train_dp = batched_train_dp.map(_transform)\n",
    "preproc_val_dp = batched_val_dp.map(_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, `preproc_train_dp` and `preproc_val_dp` are set up to produce the data that our model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  0,   0,   1,  ...,  27,  53, 109], dtype=torch.int32),\n",
       " tensor([   0,    1,    2,  ...,  515, 3289, 1688], dtype=torch.int32),\n",
       " tensor([3.5000, 4.0000, 5.0000,  ..., 4.5000, 4.5000, 5.0000]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(preproc_train_dp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining model\n",
    "\n",
    "Next, we define the model we're going to train. We'll go with a simplified two-tower model `TwoTowerModel` resembling a matrix factorization model that attempts to learn a low-rank approximation of the user-movie ratings matrix. More specifically, we want to find matrices $U \\in \\mathbb{R}^{u \\times d}$ and $M \\in \\mathbb{R}^{m \\times d}$ such that $U M^T \\approx A$, where each row in $U$ represents a user embedding of dimension $d$ and each row in $M$ a movie embedding also of dimension $d$. Once we find matrices $U$ and $M$, we can infer the rating that the $i$-th user gives the $j$-th movie as $u_i^T \\cdot m_j^T$, i.e. the dot product of the $i$-th row in $U$ and $j$-th row in $M$.\n",
    "\n",
    "`TwoTowerModel` represents $U$ and $M$ as embedding tables â€” instances of `torch.nn.Embedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerModel(torch.nn.Module):\n",
    "    def __init__(self, num_embeddings_0, num_embeddings_1, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.model_0 = torch.nn.Embedding(num_embeddings_0, embedding_dim)\n",
    "        self.model_1 = torch.nn.Embedding(num_embeddings_1, embedding_dim)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        embeddings_0 = self.model_0(input[0])\n",
    "        embeddings_1 = self.model_1(input[1])\n",
    "        return torch.sum(embeddings_0 * embeddings_1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training and evaluating model\n",
    "We're ready to train our model. Let's instantiate the model we just defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoTowerModel(\n",
    "    unique_user_ids_num, #len(contig_user_ids),\n",
    "    unique_movie_ids_num, #len(contig_movie_ids),\n",
    "    32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", instantiate our loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", and define our train and test loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dp, model, loss_fn, optimizer):\n",
    "    for batch, (users, movies, labels) in enumerate(dp):\n",
    "        pred = model((users, movies))\n",
    "        loss = loss_fn(pred, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(labels)\n",
    "            print(f\"loss: {loss:>7f}; batch: {batch}\")\n",
    "\n",
    "def test_loop(dp, model, loss_fn):\n",
    "    test_loss = 0\n",
    "    batch_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (users, movies, labels) in enumerate(dp):\n",
    "            pred = model((users, movies))\n",
    "            test_loss += loss_fn(pred, labels).item()\n",
    "            batch_count += 1\n",
    "    \n",
    "    print(f\"Test loss: {test_loss / batch_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 45.422134; batch: 0\n",
      "loss: 38.413521; batch: 100\n",
      "loss: 34.630333; batch: 200\n",
      "loss: 30.334789; batch: 300\n",
      "loss: 30.588533; batch: 400\n",
      "loss: 27.773775; batch: 500\n",
      "loss: 27.661900; batch: 600\n",
      "loss: 26.092945; batch: 700\n",
      "loss: 26.207884; batch: 800\n",
      "loss: 24.714758; batch: 900\n",
      "loss: 23.758249; batch: 1000\n",
      "loss: 23.802877; batch: 1100\n",
      "loss: 22.610825; batch: 1200\n",
      "loss: 22.531580; batch: 1300\n",
      "loss: 20.955700; batch: 1400\n",
      "loss: 21.457108; batch: 1500\n",
      "loss: 21.612574; batch: 1600\n",
      "loss: 20.770386; batch: 1700\n",
      "loss: 20.645947; batch: 1800\n",
      "loss: 20.181532; batch: 1900\n",
      "loss: 20.713175; batch: 2000\n",
      "loss: 19.967794; batch: 2100\n",
      "loss: 19.591295; batch: 2200\n",
      "loss: 20.147646; batch: 2300\n",
      "loss: 19.731419; batch: 2400\n",
      "loss: 20.435057; batch: 2500\n",
      "loss: 19.381195; batch: 2600\n",
      "loss: 20.029749; batch: 2700\n",
      "Test loss: 18.510868870354944\n",
      "loss: 17.948822; batch: 0\n",
      "loss: 17.586926; batch: 100\n",
      "loss: 17.682665; batch: 200\n",
      "loss: 17.496422; batch: 300\n",
      "loss: 17.567238; batch: 400\n",
      "loss: 17.433815; batch: 500\n",
      "loss: 17.381907; batch: 600\n",
      "loss: 17.377741; batch: 700\n",
      "loss: 17.584085; batch: 800\n",
      "loss: 17.554554; batch: 900\n",
      "loss: 16.890955; batch: 1000\n",
      "loss: 17.397343; batch: 1100\n",
      "loss: 17.477381; batch: 1200\n",
      "loss: 17.269939; batch: 1300\n",
      "loss: 17.225115; batch: 1400\n",
      "loss: 17.157743; batch: 1500\n",
      "loss: 17.039789; batch: 1600\n",
      "loss: 17.512659; batch: 1700\n",
      "loss: 17.029419; batch: 1800\n",
      "loss: 16.895119; batch: 1900\n",
      "loss: 17.151674; batch: 2000\n",
      "loss: 16.836008; batch: 2100\n",
      "loss: 16.513313; batch: 2200\n",
      "loss: 17.145594; batch: 2300\n",
      "loss: 16.958290; batch: 2400\n",
      "loss: 16.794407; batch: 2500\n",
      "loss: 16.675030; batch: 2600\n",
      "loss: 16.970404; batch: 2700\n",
      "Test loss: 16.452521972406924\n",
      "loss: 16.108639; batch: 0\n",
      "loss: 16.002983; batch: 100\n",
      "loss: 15.970982; batch: 200\n",
      "loss: 15.741371; batch: 300\n",
      "loss: 15.968441; batch: 400\n",
      "loss: 15.719864; batch: 500\n",
      "loss: 15.813468; batch: 600\n",
      "loss: 15.439825; batch: 700\n",
      "loss: 15.531449; batch: 800\n",
      "loss: 15.651622; batch: 900\n",
      "loss: 15.750284; batch: 1000\n",
      "loss: 15.830811; batch: 1100\n",
      "loss: 16.401840; batch: 1200\n",
      "loss: 15.583506; batch: 1300\n",
      "loss: 15.687699; batch: 1400\n",
      "loss: 15.712193; batch: 1500\n",
      "loss: 15.287286; batch: 1600\n",
      "loss: 15.397345; batch: 1700\n",
      "loss: 15.260979; batch: 1800\n",
      "loss: 15.368156; batch: 1900\n",
      "loss: 15.644093; batch: 2000\n",
      "loss: 15.435254; batch: 2100\n",
      "loss: 15.130157; batch: 2200\n",
      "loss: 15.641044; batch: 2300\n",
      "loss: 15.730799; batch: 2400\n",
      "loss: 15.941284; batch: 2500\n",
      "loss: 15.488480; batch: 2600\n",
      "loss: 15.925428; batch: 2700\n",
      "Test loss: 15.488299245148703\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "for __ in range(epochs):\n",
    "    train_loop(preproc_train_dp, model, loss_fn, optimizer)\n",
    "    test_loop(preproc_val_dp, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a trained model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finding similar movies\n",
    "For kicks, let's see if we can use our model's trained embeddings to find movies that are most similar to some query movie. In theory, movies with embeddings that are similar should themselves be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id_to_title_genre = _transform._movie_id_to_title_genre\n",
    "contig_to_movie_id = {v: k for k, v in _transform._contig_movie_ids.items()}\n",
    "contig_movie_ids = _transform._contig_movie_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contig_to_movie_id = {v: k for k, v in contig_movie_ids.items()}\n",
    "\n",
    "def get_topk_sim_movies(movie_id, k=20):\n",
    "    embedding = model.model_1(torch.tensor([contig_movie_ids[movie_id]]))\n",
    "    movie_embeddings = model.get_parameter(\"model_1.weight\")\n",
    "    movie_similarities = torch.sum(embedding * movie_embeddings, axis=1) / torch.maximum(torch.norm(embedding) * torch.norm(movie_embeddings, dim=1), torch.ones(movie_embeddings.shape[0]) * 1e-12)\n",
    "    topk_sim = torch.topk(movie_similarities, 20)\n",
    "    contig_ids = topk_sim.indices.tolist()\n",
    "    return [\n",
    "        (*movie_id_to_title_genre[contig_to_movie_id[movie_id]], contig_to_movie_id[movie_id]) \n",
    "        for movie_id in contig_ids\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Drive (2011)', 'Crime|Drama|Film-Noir|Thriller', 88129),\n",
       " ('The Hunger Games (2012)', 'Action|Adventure|Drama|Sci-Fi|Thriller', 91500),\n",
       " ('Social Network, The (2010)', 'Drama', 80463),\n",
       " ('Prometheus (2012)', 'Action|Horror|Sci-Fi|IMAX', 94864),\n",
       " ('Harry Potter and the Order of the Phoenix (2007)',\n",
       "  'Adventure|Drama|Fantasy|IMAX',\n",
       "  54001),\n",
       " ('Looper (2012)', 'Action|Crime|Sci-Fi', 96610),\n",
       " ('The Butterfly Effect (2004)', 'Drama|Sci-Fi|Thriller', 7254),\n",
       " ('In Bruges (2008)', 'Comedy|Crime|Drama|Thriller', 57669),\n",
       " ('Kiss Kiss Bang Bang (2005)', 'Comedy|Crime|Mystery|Thriller', 38061),\n",
       " ('X-Men: First Class (2011)', 'Action|Adventure|Sci-Fi|Thriller|War', 87232),\n",
       " ('Old Boy (2003)', 'Mystery|Thriller', 27773),\n",
       " ('Lord of War (2005)', 'Action|Crime|Drama|Thriller|War', 36529),\n",
       " ('John Wick (2014)', 'Action|Thriller', 115149),\n",
       " ('Birdman: Or (The Unexpected Virtue of Ignorance) (2014)',\n",
       "  'Comedy|Drama',\n",
       "  112183),\n",
       " ('District 9 (2009)', 'Mystery|Sci-Fi|Thriller', 70286),\n",
       " ('Moon (2009)', 'Drama|Mystery|Sci-Fi|Thriller', 68237),\n",
       " ('Source Code (2011)', 'Action|Drama|Mystery|Sci-Fi|Thriller', 85414),\n",
       " ('There Will Be Blood (2007)', 'Drama|Western', 56782),\n",
       " ('Zodiac (2007)', 'Crime|Drama|Thriller', 51540),\n",
       " ('Watchmen (2009)', 'Action|Drama|Mystery|Sci-Fi|Thriller|IMAX', 60684)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drive\n",
    "get_topk_sim_movies(88129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lost in Translation (2003)', 'Comedy|Drama|Romance', 6711),\n",
       " ('Who Framed Roger Rabbit? (1988)',\n",
       "  'Adventure|Animation|Children|Comedy|Crime|Fantasy|Mystery',\n",
       "  2987),\n",
       " ('Top Gun (1986)', 'Action|Romance', 1101),\n",
       " ('Memento (2000)', 'Mystery|Thriller', 4226),\n",
       " ('Shining, The (1980)', 'Horror', 1258),\n",
       " ('High Fidelity (2000)', 'Comedy|Drama|Romance', 3481),\n",
       " ('Jaws (1975)', 'Action|Horror', 1387),\n",
       " ('Psycho (1960)', 'Crime|Horror', 1219),\n",
       " ('Groundhog Day (1993)', 'Comedy|Fantasy|Romance', 1265),\n",
       " ('Exorcist, The (1973)', 'Horror|Mystery', 1997),\n",
       " ('Ice Age (2002)', 'Adventure|Animation|Children|Comedy', 5218),\n",
       " ('Platoon (1986)', 'Drama|War', 1090),\n",
       " ('Wizard of Oz, The (1939)', 'Adventure|Children|Fantasy|Musical', 919),\n",
       " ('Seven Samurai (Shichinin no samurai) (1954)',\n",
       "  'Action|Adventure|Drama',\n",
       "  2019),\n",
       " ('Scream (1996)', 'Comedy|Horror|Mystery|Thriller', 1407),\n",
       " ('V for Vendetta (2006)', 'Action|Sci-Fi|Thriller|IMAX', 44191),\n",
       " ('Willy Wonka & the Chocolate Factory (1971)',\n",
       "  'Children|Comedy|Fantasy|Musical',\n",
       "  1073),\n",
       " ('Titanic (1997)', 'Drama|Romance', 1721),\n",
       " ('Deer Hunter, The (1978)', 'Drama|War', 1263),\n",
       " ('Annie Hall (1977)', 'Comedy|Romance', 1230)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lost in Translation\n",
    "get_topk_sim_movies(6711)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ratatouille (2007)', 'Animation|Children|Drama', 50872),\n",
       " ('Iron Man (2008)', 'Action|Adventure|Sci-Fi', 59315),\n",
       " ('Ice Age (2002)', 'Adventure|Animation|Children|Comedy', 5218),\n",
       " ('Dark Knight Rises, The (2012)', 'Action|Adventure|Crime|IMAX', 91529),\n",
       " ('Incredibles, The (2004)',\n",
       "  'Action|Adventure|Animation|Children|Comedy',\n",
       "  8961),\n",
       " ('Shrek 2 (2004)',\n",
       "  'Adventure|Animation|Children|Comedy|Musical|Romance',\n",
       "  8360),\n",
       " ('Interstellar (2014)', 'Sci-Fi|IMAX', 109487),\n",
       " (\"Ocean's Twelve (2004)\", 'Action|Comedy|Crime|Thriller', 8984),\n",
       " ('District 9 (2009)', 'Mystery|Sci-Fi|Thriller', 70286),\n",
       " ('Toy Story 3 (2010)',\n",
       "  'Adventure|Animation|Children|Comedy|Fantasy|IMAX',\n",
       "  78499),\n",
       " ('Collateral (2004)', 'Action|Crime|Drama|Thriller', 8798),\n",
       " ('Finding Nemo (2003)', 'Adventure|Animation|Children|Comedy', 6377),\n",
       " ('X-Men: First Class (2011)', 'Action|Adventure|Sci-Fi|Thriller|War', 87232),\n",
       " ('Sherlock Holmes (2009)', 'Action|Crime|Mystery|Thriller', 73017),\n",
       " ('Last Samurai, The (2003)', 'Action|Adventure|Drama|War', 7143),\n",
       " ('Dark Knight, The (2008)', 'Action|Crime|Drama|IMAX', 58559),\n",
       " ('Taken (2008)', 'Action|Crime|Drama|Thriller', 59369),\n",
       " ('Burn After Reading (2008)', 'Comedy|Crime|Drama', 61323),\n",
       " ('Lord of War (2005)', 'Action|Crime|Drama|Thriller|War', 36529),\n",
       " ('Annie Hall (1977)', 'Comedy|Romance', 1230)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ratatouille\n",
    "get_topk_sim_movies(50872)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think? Can we do better?"
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "552815895724328"
  },
  "disseminate_notebook_info": {
   "bento_version": "20210627-210324",
   "description": "",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "deps": [
     "//caffe2/caffe2/fb/ifbpy:all_pytorch_and_caffe2_deps",
     "//github/third-party/PyTorchLightning/pytorch-lightning:lib"
    ],
    "external_deps": []
   },
   "no_uii": true,
   "notebook_number": "954867",
   "others_can_edit": false,
   "reviewers": "",
   "revision_id": "489172452146612",
   "tags": "",
   "tasks": "",
   "title": "torchrec movielens"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
